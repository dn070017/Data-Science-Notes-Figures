{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "```\n",
    "bokeh serve --show widget_gradient_descent.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent 1D\n",
    "\n",
    "Suppose we have a one variable linear function $\\mathbf{y}=3.145\\mathbf{x}$, where $y$ and $x$ is a $m$ length vector. Now we would like to construct a model to fit the above-mentioned function: $\\mathbf{\\hat{y}}=\\mathbf{x}w$. It is trivial to see that $w=3.145$ gives us the exact function of $\\mathbf{y}$. Here, we will use a __gradient descent__ algorithm to approximate the solution.\n",
    "\n",
    "![Gradient Descent 1D](assets/gradient_descent1d.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal((num_samples, 1), mean=0, stddev=1)\n",
    "y = 3.145 * x\n",
    "\n",
    "w = tf.Variable(0.)\n",
    "y_hat = w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we defined the __objective function__ to be the square deviation between $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "L&=\\sum\\limits_{i=1}^{m}(y_i - \\hat{y}_i)^2\\\\\n",
    " &=\\sum\\limits_{i=1}^{m}(y_i - wx_i)^2\n",
    "\\end{align}\n",
    "$$\n",
    "The objective functin can be written as $(\\mathbf{y}-\\mathbf{Xw})^TI(\\mathbf{y}-\\mathbf{Xw})$. From the previous section, we know that the gradient of $L$ with respect to $w$ is:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}L(\\mathbf{\\hat{y}})=[-2(\\mathbf{y}-\\mathbf{\\hat{y}})^T\\mathbf{X}]^T\n",
    "$$\n",
    "Note that the direction of the gradient will always point way from the weight that yield local minimum of the loss surface, therefore, we can update the weight on the opposite direction of $\\nabla_{\\mathbf{w}}L(\\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)} - \\eta\\nabla_{\\mathbf{w}}L(\\mathbf{\\hat{y}})\n",
    "$$\n",
    "where $\\eta$ is a hyperparameter called __learning rate__, which is used to control the magnitude of each updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iteration: 1    weight: 2.204    loss: 115.505\niteration: 2    weight: 2.863    loss:  10.349\niteration: 3    weight: 3.061    loss:   0.927\niteration: 4    weight: 3.120    loss:   0.083\niteration: 5    weight: 3.137    loss:   0.007\niteration: 6    weight: 3.143    loss:   0.001\niteration: 7    weight: 3.144    loss:   0.000\niteration: 8    weight: 3.145    loss:   0.000\niteration: 9    weight: 3.145    loss:   0.000\niteration:10    weight: 3.145    loss:   0.000\n"
    }
   ],
   "source": [
    "eta = 0.03\n",
    "for i in range(0, 10):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(w)\n",
    "        loss = tf.reduce_sum(tf.pow(y - w * x, 2))\n",
    "    gradient = g.gradient(loss, w)\n",
    "    w = w.assign(w - eta * gradient) \n",
    "    print(f'iteration:{i+1:>2}    weight: {w.numpy():>.3f}    loss: {loss.numpy():>7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient Descent 2D\n",
    "Now let's look into multiple variables regression:  $\\mathbf{y}=3.145\\mathbf{x}_1 + 6.323\\mathbf{x}_2$, we whould like to find $\\hat{y}=w_1\\mathbf{x}_1+w_2\\mathbf{x}_2$ that approximate this function:\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{w} =\n",
    "\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2}\\\\\n",
    "    x_{2,1} & x_{2,2}\\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    w_{1}   \\\\\n",
    "    w_{2}   \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The square loss function then become:\n",
    "$$\n",
    "\\begin{align}\n",
    "L&=\\sum\\limits_{i=1}^{m}(y_i - \\hat{y}_i)^2\\\\\n",
    " &=\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{2}(y_i - w_jx_{ij})^2\n",
    "\\end{align}\n",
    "$$\n",
    "The gradient of $L$ with respect to $\\mathbf{w}$ remains the same, but with additional dimension:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}L(\\mathbf{\\hat{y}})=[-2(\\mathbf{y}-\\mathbf{\\hat{y}})^T\\mathbf{X}]^T\n",
    "$$\n",
    "![Gradient Descent 2D](assets/gradient_descent2d.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iteration: 1    weight 1: 4.542    weight 2: 4.901    loss: 754.581\niteration: 2    weight 1: 2.929    weight 2: 5.802    loss:  58.888\niteration: 3    weight 1: 3.249    weight 2: 6.202    loss:   4.620\niteration: 4    weight 1: 3.130    weight 2: 6.280    loss:   0.364\niteration: 5    weight 1: 3.153    weight 2: 6.313    loss:   0.029\niteration: 6    weight 1: 3.144    weight 2: 6.319    loss:   0.002\niteration: 7    weight 1: 3.146    weight 2: 6.322    loss:   0.000\niteration: 8    weight 1: 3.145    weight 2: 6.323    loss:   0.000\niteration: 9    weight 1: 3.145    weight 2: 6.323    loss:   0.000\niteration:10    weight 1: 3.145    weight 2: 6.323    loss:   0.000\n"
    }
   ],
   "source": [
    "\n",
    "eta = 0.03\n",
    "num_samples = 20\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal((num_samples, 2), mean=0, stddev=1)\n",
    "y = tf.reshape(3.145 * x[:, 0] +  6.323 * x[:, 1], (-1, 1))\n",
    "\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(w)\n",
    "        loss = tf.reduce_sum(tf.pow(y - tf.matmul(x, w), 2))\n",
    "    gradient = g.gradient(loss, w)\n",
    "    w = w.assign(w - eta * gradient) \n",
    "    print(f'iteration:{i+1:>2}    weight 1: {w.numpy()[0, 0]:>.3f}    weight 2: {w.numpy()[1, 0]:>.3f}    loss: {loss.numpy():>7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch and Batch\n",
    "When we compute the loss, we can either consider every sample or a subset of samples at a time. We call the iteration that every sample in the dataset has been used to update the parameters an __epoch__. In each epoch, we can split the samples into different __batches__. \n",
    "* __Mini-batch gradient descent__: updates the parameters consider a batch of samples at a time. \n",
    "* __Stochastic gradient descent (SGD)__: update the parameters consider one sample at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def mini_batch_gradient_descent(x, y, w, num_epochs, batch_size, eta=0.03, example=1):\n",
    "    num_samples = x.shape[0]\n",
    "\n",
    "    for i in range(0, num_epochs):\n",
    "\n",
    "        indices = tf.range(start=0, limit=num_samples, dtype=tf.int32)\n",
    "        tf.random.set_seed(i)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "        shuffled_x = tf.gather(x, shuffled_indices)\n",
    "        shuffled_y = tf.gather(y, shuffled_indices)\n",
    "\n",
    "        batches = [batch_size] * (num_samples // batch_size)\n",
    "        if num_samples % batch_size != 0:\n",
    "            batches.extend([num_samples % batch_size])\n",
    "\n",
    "        for j, (batch_x, batch_y) in enumerate(zip(tf.split(shuffled_x, batches, axis=0), tf.split(shuffled_y, batches, axis=0))):\n",
    "            with tf.GradientTape() as g:\n",
    "                g.watch(w)\n",
    "                if example == 1:\n",
    "                    loss = tf.reduce_sum(tf.pow(batch_y - tf.matmul(batch_x, w), 2))\n",
    "                elif example == 2:\n",
    "                    loss = tf.reduce_sum(tf.pow(batch_y - tf.matmul(batch_x, tf.pow(w, 2)), 2))\n",
    "            gradient = g.gradient(loss, w)\n",
    "            w = w.assign(w - eta * gradient) \n",
    "            print(f'epoch:{i+1:>2}    batch:{j+1:>2}     weight 1: {w.numpy()[0, 0]:>.3f}    weight 2: {w.numpy()[1, 0]:>.3f}    loss: {loss.numpy():>7.3f}')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 1    batch: 1     weight 1: 2.133    weight 2: 1.662    loss: 286.972\nepoch: 1    batch: 2     weight 1: 3.116    weight 2: 3.946    loss: 194.002\nepoch: 2    batch: 1     weight 1: 3.037    weight 2: 4.373    loss:  16.866\nepoch: 2    batch: 2     weight 1: 3.350    weight 2: 5.456    loss:  35.755\nepoch: 3    batch: 1     weight 1: 3.191    weight 2: 5.726    loss:   4.440\nepoch: 3    batch: 2     weight 1: 3.236    weight 2: 5.972    loss:   2.423\nepoch: 4    batch: 1     weight 1: 3.188    weight 2: 6.110    loss:   0.875\nepoch: 4    batch: 2     weight 1: 3.176    weight 2: 6.177    loss:   0.248\nepoch: 5    batch: 1     weight 1: 3.182    weight 2: 6.231    loss:   0.128\nepoch: 5    batch: 2     weight 1: 3.149    weight 2: 6.263    loss:   0.070\nepoch: 6    batch: 1     weight 1: 3.147    weight 2: 6.286    loss:   0.023\nepoch: 6    batch: 2     weight 1: 3.149    weight 2: 6.298    loss:   0.008\nepoch: 7    batch: 1     weight 1: 3.144    weight 2: 6.307    loss:   0.004\nepoch: 7    batch: 2     weight 1: 3.147    weight 2: 6.313    loss:   0.002\nepoch: 8    batch: 1     weight 1: 3.147    weight 2: 6.317    loss:   0.001\nepoch: 8    batch: 2     weight 1: 3.145    weight 2: 6.319    loss:   0.000\nepoch: 9    batch: 1     weight 1: 3.146    weight 2: 6.321    loss:   0.000\nepoch: 9    batch: 2     weight 1: 3.145    weight 2: 6.321    loss:   0.000\nepoch:10    batch: 1     weight 1: 3.145    weight 2: 6.322    loss:   0.000\nepoch:10    batch: 2     weight 1: 3.145    weight 2: 6.322    loss:   0.000\n"
    }
   ],
   "source": [
    "eta = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "num_samples = 20\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal((num_samples, 2), mean=0, stddev=1)\n",
    "y = tf.reshape(3.145 * x[:, 0] +  6.323 * x[:, 1], (-1, 1))\n",
    "\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "\n",
    "mini_batch_gradient_descent(x, y, w, num_epochs, batch_size, eta) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the loss function is a convex function with respect to the parameters, gradient descent is guaranteed to find the optimal parameters that minimize the loss globally (if the learning rate is properly configured). However, this assumption does not always hold. For example, if we defined a non-linear transformation on $\\mathbf{w}$: $\\hat{y}=w_1^2\\mathbf{x}_1+w_2^2\\mathbf{x}_2$:\n",
    "$$\n",
    "\\begin{align}\n",
    "L&=\\sum\\limits_{i=1}^{m}(y_i - \\hat{y}_i)^2\\\\\n",
    " &=\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{2}(y_i - w_j^2x_{ij})^2\n",
    "\\end{align}\n",
    "$$\n",
    "The gradient of $L$ with respect to $w$ then become:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}L(\\hat{y})=-4(\\mathbf{y} - \\hat{y})^T\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "The gradient descent optimization might not be able to find the optimal parameters that globally minimize the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 1    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 8055.223\nepoch: 1    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 14630.254\nepoch: 2    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 5639.693\nepoch: 2    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 17045.783\nepoch: 3    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 8882.165\nepoch: 3    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 13803.312\nepoch: 4    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 12194.377\nepoch: 4    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 10491.100\nepoch: 5    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 13091.315\nepoch: 5    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 9594.162\nepoch: 6    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 11150.024\nepoch: 6    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 11535.453\nepoch: 7    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 9831.545\nepoch: 7    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 12853.932\nepoch: 8    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 12608.361\nepoch: 8    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 10077.116\nepoch: 9    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 14355.027\nepoch: 9    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 8330.450\nepoch:10    batch: 1     weight 1: 0.000    weight 2: 0.000    loss: 5318.655\nepoch:10    batch: 2     weight 1: 0.000    weight 2: 0.000    loss: 17366.820\n"
    }
   ],
   "source": [
    "eta = 1e-4\n",
    "batch_size = 10\n",
    "num_samples = 20\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal((num_samples, 2), mean=0, stddev=1)\n",
    "y = tf.reshape(3.145 ** 2 * x[:, 0] +  6.323 ** 2 * x[:, 1], (-1, 1))\n",
    "\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "\n",
    "mini_batch_gradient_descent(x, y, w, 10, batch_size, eta, example=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually solve this by proper weight initialization or applying the optimizers, which we will explain in more details in other exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 1    batch: 1     weight 1: 3.013    weight 2: 6.041    loss:  77.747\nepoch: 1    batch: 2     weight 1: 3.028    weight 2: 6.110    loss: 109.897\nepoch: 2    batch: 1     weight 1: 3.039    weight 2: 6.128    loss:  25.780\nepoch: 2    batch: 2     weight 1: 3.051    weight 2: 6.186    loss:  63.831\nepoch: 3    batch: 1     weight 1: 3.058    weight 2: 6.207    loss:  17.849\nepoch: 3    batch: 2     weight 1: 3.068    weight 2: 6.235    loss:  20.875\nepoch: 4    batch: 1     weight 1: 3.075    weight 2: 6.254    loss:  11.259\nepoch: 4    batch: 2     weight 1: 3.081    weight 2: 6.267    loss:   6.715\nepoch: 5    batch: 1     weight 1: 3.087    weight 2: 6.281    loss:   5.718\nepoch: 5    batch: 2     weight 1: 3.092    weight 2: 6.288    loss:   3.027\nepoch: 6    batch: 1     weight 1: 3.097    weight 2: 6.295    loss:   2.363\nepoch: 6    batch: 2     weight 1: 3.101    weight 2: 6.301    loss:   2.029\nepoch: 7    batch: 1     weight 1: 3.106    weight 2: 6.305    loss:   1.479\nepoch: 7    batch: 2     weight 1: 3.109    weight 2: 6.310    loss:   0.919\nepoch: 8    batch: 1     weight 1: 3.111    weight 2: 6.314    loss:   0.760\nepoch: 8    batch: 2     weight 1: 3.115    weight 2: 6.315    loss:   0.616\nepoch: 9    batch: 1     weight 1: 3.116    weight 2: 6.318    loss:   0.398\nepoch: 9    batch: 2     weight 1: 3.120    weight 2: 6.319    loss:   0.461\nepoch:10    batch: 1     weight 1: 3.122    weight 2: 6.319    loss:   0.374\nepoch:10    batch: 2     weight 1: 3.124    weight 2: 6.321    loss:   0.187\n"
    }
   ],
   "source": [
    "eta = 1e-4\n",
    "w = tf.Variable(tf.convert_to_tensor([[3.], [6.]]))\n",
    "\n",
    "mini_batch_gradient_descent(x, y, w, 10, batch_size, eta, example=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596520911449",
   "display_name": "Python 3.7.7 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}