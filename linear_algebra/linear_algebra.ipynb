{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages, Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class ANSI():\n",
    "    BOLD = '\\033[1m'\n",
    "    END  = '\\033[0m' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Declaration\n",
    "We often use $\\mathbf{X}$ to denote the feature matrix of the samples, where the where $x_{i,j}$ is the j-th feature of the i-th sample. Note that it is common to have higher dimensional feature tensor.\n",
    "$$\n",
    "\\mathbf{X}=[x_{i,j}]=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1m[tf.eye]\u001b[0m Identity matrix\n[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n\n\u001b[1m[tf.ones]\u001b[0m Tensor full of ones\n[[1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]]\n\n\u001b[1m[tf.zeros]\u001b[0m Tensor full of zeros\n[[0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]\n\n\u001b[1m[tf.ones_like]\u001b[0m Tensor full of ones with same dimension of provided matrix\n[[1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1.]]\n\n\u001b[1m[tf.zeros_like]\u001b[0m Tensor full of zeros with same dimension of provided matrix\n[[0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]\n\n\u001b[1m[tf.random.normal]\u001b[0m Tensor with random values from normal distribution\n[[ 0.7555313   0.21146102 -0.20984747 -0.5180186  -0.6184139 ]\n [ 0.23513651 -0.00698744  0.59442914  0.30126667  0.29985556]\n [-0.35285595 -0.21648772  0.39681226 -0.3487463  -0.4799166 ]\n [-0.45034844 -0.18040527 -0.11188658  0.15191923  0.26076272]]\n\n\u001b[1m[tf.random.uniform]\u001b[0m Tensor with random values\n[[-0.41604972 -0.5868671   0.07078147  0.12251496 -0.16665101]\n [ 0.6156559  -0.0135498   0.9962585   0.3934703  -0.7492528 ]\n [ 0.4196334   0.32483125  0.14451313 -0.27049303 -0.15896344]\n [ 0.26011395  0.827626    0.3232944   0.6669471  -0.83208394]]\n\n\u001b[1m[tf.convert_to_tensor]\u001b[0m Tensor from List or Numpy array\n[[-0.41604972 -0.5868671   0.07078147  0.12251496 -0.16665101]\n [ 0.6156559  -0.0135498   0.9962585   0.3934703  -0.7492528 ]\n [ 0.4196334   0.32483125  0.14451313 -0.27049303 -0.15896344]\n [ 0.26011395  0.827626    0.3232944   0.6669471  -0.83208394]]\n\n"
    }
   ],
   "source": [
    "print(f'{ANSI.BOLD}[tf.eye]{ANSI.END} Identity matrix\\n{tf.eye(4)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.ones]{ANSI.END} Tensor full of ones\\n{tf.ones((4, 5))}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.zeros]{ANSI.END} Tensor full of zeros\\n{tf.zeros((4, 5))}\\n')\n",
    "X = tf.ones((4, 5))\n",
    "print(f'{ANSI.BOLD}[tf.ones_like]{ANSI.END} Tensor full of ones with same dimension of provided matrix\\n{tf.ones_like(X)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.zeros_like]{ANSI.END} Tensor full of zeros with same dimension of provided matrix\\n{tf.zeros_like(X)}\\n')\n",
    "tf.random.set_seed(0)\n",
    "X = tf.random.normal((4, 5), mean=0, stddev=0.5)\n",
    "print(f'{ANSI.BOLD}[tf.random.normal]{ANSI.END} Tensor with random values from normal distribution\\n{X}\\n')\n",
    "tf.random.set_seed(0)\n",
    "X = tf.random.uniform((4, 5), minval=-1, maxval=1)\n",
    "print(f'{ANSI.BOLD}[tf.random.uniform]{ANSI.END} Tensor with random values\\n{X}\\n')\n",
    "X = tf.convert_to_tensor(X.numpy())\n",
    "print(f'{ANSI.BOLD}[tf.convert_to_tensor]{ANSI.END} Tensor from List or Numpy array\\n{X}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Properties\n",
    "Here are some commonly used important properties of Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1m[X.device]\u001b[0m: /job:localhost/replica:0/task:0/device:CPU:0\n\u001b[1m[X.dtype]\u001b[0m: {tf.float32}\n\u001b[1m[X.shape]\u001b[0m: (4, 5)\n"
    }
   ],
   "source": [
    "print(f'{ANSI.BOLD}[X.device]{ANSI.END}: {X.device}')\n",
    "print(f'{ANSI.BOLD}[X.dtype]{ANSI.END}:', {X.dtype})\n",
    "print(f'{ANSI.BOLD}[X.shape]{ANSI.END}: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Declaration\n",
    "Note that the declared Tensor object is constant (i.e the values cannot be changed directly). We often use Tensor object for the feature tensor. We can defined Variable object for mutable tensors. We often use Variable object for the __trainable__ parameters in the neural networks:<br><br>\n",
    "$$\n",
    "\\mathbf{w}=[w_{j}]=\\begin{bmatrix}\n",
    "    w_{1} \\\\\n",
    "    w_{2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    w_{n} \\\\\n",
    "    \\end{bmatrix}, b=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1m[tf.Variable]\u001b[0m Declare Variable\n[0.]\n\n\u001b[1m[V.assign, V.assign_add, V.assign_sub]\u001b[0m Replace (add, subtract) values with given tensor (only change reference if read_value=True)\nWeight (before)\n[[-0.41604972]\n [-0.5868671 ]\n [ 0.07078147]\n [ 0.12251496]\n [-0.16665101]]\nWeight (after)\n[[-0.6697383 ]\n [ 0.80296254]\n [ 0.26194835]\n [-0.13090777]\n [-0.41612196]]\n\n\u001b[1m[V.scatter_nd_update, V.scatter_nd_add, V.scatter_nd_sub]\u001b[0m Replace (add, subtract) values in the given indices (change reference and also return new Variable)\nWeight (before)\n[[-0.41604972]\n [-0.5868671 ]\n [ 0.07078147]\n [ 0.12251496]\n [-0.16665101]]\nWeight (after)\n[[ 0.        ]\n [-0.5868671 ]\n [ 0.07078147]\n [ 0.12251496]\n [-0.16665101]]\nReturn Tensor\n[[ 0.        ]\n [-0.5868671 ]\n [ 0.07078147]\n [ 0.12251496]\n [-0.16665101]]\nDo reference and return Tensor have the same values: True\nDo reference and return Tensor point to same object: False\n"
    }
   ],
   "source": [
    "b = tf.Variable([0], dtype=tf.dtypes.float32)\n",
    "print(f'{ANSI.BOLD}[tf.Variable]{ANSI.END} Declare Variable\\n{b.numpy()}\\n')\n",
    "\n",
    "print(f'{ANSI.BOLD}[V.assign, V.assign_add, V.assign_sub]{ANSI.END} Replace (add, subtract) values with given tensor (only change reference if read_value=True)')\n",
    "tf.random.set_seed(0)\n",
    "w = tf.Variable(tf.random.uniform((5, 1), minval=-1, maxval=1))\n",
    "print(f'Weight (before)\\n{w.numpy()}')\n",
    "tf.random.set_seed(1)\n",
    "tmp = tf.Variable(tf.random.uniform((5, 1), minval=-1, maxval=1))\n",
    "w.assign(tmp)\n",
    "print(f'Weight (after)\\n{w.numpy()}\\n')\n",
    "\n",
    "print(f'{ANSI.BOLD}[V.scatter_nd_update, V.scatter_nd_add, V.scatter_nd_sub]{ANSI.END} Replace (add, subtract) values in the given indices (change reference and also return new Variable)')\n",
    "tf.random.set_seed(0)\n",
    "w = tf.Variable(tf.random.uniform((5, 1), minval=-1, maxval=1))\n",
    "indices = tf.constant([[0, 0]])\n",
    "updates = tf.constant([0], dtype=tf.dtypes.float32)\n",
    "print(f'Weight (before)\\n{w.numpy()}')\n",
    "updated = w.scatter_nd_update(indices, updates).numpy()\n",
    "print(f'Weight (after)\\n{w.numpy()}')\n",
    "print(f'Return Tensor\\n{updated}')\n",
    "print(f'Do reference and return Tensor have the same values: {all(w == updated)}')\n",
    "print(f'Do reference and return Tensor point to same object: {w is updated}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1m[tf.reshape]\u001b[0m Reshape the Tensor\n[[-0.41604972 -0.5868671 ]\n [ 0.07078147  0.12251496]\n [-0.16665101  0.6156559 ]\n [-0.0135498   0.9962585 ]\n [ 0.3934703  -0.7492528 ]\n [ 0.4196334   0.32483125]\n [ 0.14451313 -0.27049303]\n [-0.15896344  0.26011395]\n [ 0.827626    0.3232944 ]\n [ 0.6669471  -0.83208394]]\n\n\u001b[1m[tf.transpose]\u001b[0m Swap dimension of the Tensor\n[[-0.41604972  0.6156559   0.4196334   0.26011395]\n [-0.5868671  -0.0135498   0.32483125  0.827626  ]\n [ 0.07078147  0.9962585   0.14451313  0.3232944 ]\n [ 0.12251496  0.3934703  -0.27049303  0.6669471 ]\n [-0.16665101 -0.7492528  -0.15896344 -0.83208394]]\n\n\u001b[1m[tf.split]\u001b[0m Split the Tensor into list of Tensors\nA\n[[-0.41604972 -0.5868671   0.07078147  0.12251496 -0.16665101]\n [ 0.6156559  -0.0135498   0.9962585   0.3934703  -0.7492528 ]]\nB\n[[ 0.4196334   0.32483125  0.14451313 -0.27049303 -0.15896344]\n [ 0.26011395  0.827626    0.3232944   0.6669471  -0.83208394]]\n\n\u001b[1m[tf.concat]\u001b[0m Concat list of Tensors into one\n[[-0.41604972 -0.5868671   0.07078147  0.12251496 -0.16665101]\n [ 0.6156559  -0.0135498   0.9962585   0.3934703  -0.7492528 ]\n [ 0.4196334   0.32483125  0.14451313 -0.27049303 -0.15896344]\n [ 0.26011395  0.827626    0.3232944   0.6669471  -0.83208394]]\n\n\u001b[1m[tf.stack]\u001b[0m Concat list of Tensors on a new axis\n[[[-0.41604972 -0.5868671   0.07078147  0.12251496 -0.16665101]\n  [ 0.6156559  -0.0135498   0.9962585   0.3934703  -0.7492528 ]]\n\n [[ 0.4196334   0.32483125  0.14451313 -0.27049303 -0.15896344]\n  [ 0.26011395  0.827626    0.3232944   0.6669471  -0.83208394]]]\n\n\u001b[1m[tf.matmul, operators]\u001b[0m Xw + b\n[[ 0.39220545]\n [ 0.2515383 ]\n [-0.18705195]\n [-0.24244456]]\n\n\u001b[1m[tf.sort]\u001b[0m Sort the Tensor\n[[-0.5868671  -0.41604972 -0.16665101  0.07078147  0.12251496]\n [-0.7492528  -0.0135498   0.3934703   0.6156559   0.9962585 ]\n [-0.27049303 -0.15896344  0.14451313  0.32483125  0.4196334 ]\n [-0.83208394  0.26011395  0.3232944   0.6669471   0.827626  ]]\n\n\u001b[1m[tf.argsort]\u001b[0m Get the order of the Tensor\n[[1 0 4 2 3]\n [4 1 3 0 2]\n [3 4 2 1 0]\n [4 0 2 3 1]]\n\n\u001b[1m[tf.reduce_min, tf,reduce_max, tf.reduce_mean, tf_reduce_sum]\u001b[0m Compute min, max, mean and sum of the Tensor \n[-0.5868671  -0.7492528  -0.27049303 -0.83208394]\n\n\u001b[1m[tfp.stats.percentile]\u001b[0m Compute percentile of the Tensor\n[-0.16665101  0.3934703   0.14451313  0.3232944 ]\n\n\u001b[1m[tf.pow, tf.log]\u001b[0m Compute power or log of Tensor\n[[1.7309737e-01 3.4441298e-01 5.0100163e-03 1.5009916e-02 2.7772559e-02]\n [3.7903219e-01 1.8359721e-04 9.9253100e-01 1.5481886e-01 5.6137973e-01]\n [1.7609218e-01 1.0551534e-01 2.0884044e-02 7.3166482e-02 2.5269376e-02]\n [6.7659266e-02 6.8496478e-01 1.0451927e-01 4.4481847e-01 6.9236368e-01]]\n\n\u001b[1m[tf.squeeze]\u001b[0m Remove dimensions with 1 length\n[ 0.         -0.5868671   0.07078147  0.12251496 -0.16665101]\n\n\u001b[1m[tf.expand_dims]\u001b[0m Expand dimension \n[[ 0.        ]\n [-0.5868671 ]\n [ 0.07078147]\n [ 0.12251496]\n [-0.16665101]]\n\n\u001b[1m[tf.where]\u001b[0m Replace values if condition is (not) met\n[[-0.41604972 -0.5868671   0.          0.         -0.16665101]\n [ 0.         -0.0135498   0.          0.         -0.7492528 ]\n [ 0.          0.          0.         -0.27049303 -0.15896344]\n [ 0.          0.          0.          0.         -0.83208394]]\n"
    }
   ],
   "source": [
    "print(f'{ANSI.BOLD}[tf.reshape]{ANSI.END} Reshape the Tensor\\n{tf.reshape(X, (10, 2))}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.transpose]{ANSI.END} Swap dimension of the Tensor\\n{tf.transpose(X, [1, 0])}\\n')\n",
    "A, B = tf.split(X, [2, 2], axis=0)\n",
    "print(f'{ANSI.BOLD}[tf.split]{ANSI.END} Split the Tensor into list of Tensors\\nA\\n{A}\\nB\\n{B}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.concat]{ANSI.END} Concat list of Tensors into one\\n{tf.concat([A, B], axis=0)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.stack]{ANSI.END} Concat list of Tensors on a new axis\\n{tf.stack([A, B], axis=0)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.matmul, operators]{ANSI.END} Xw + b\\n{tf.matmul(X, w) + b}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.sort]{ANSI.END} Sort the Tensor\\n{tf.sort(X, axis=1)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.argsort]{ANSI.END} Get the order of the Tensor\\n{tf.argsort(X, axis=1)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.reduce_min, tf,reduce_max, tf.reduce_mean, tf_reduce_sum]{ANSI.END} Compute min, max, mean and sum of the Tensor \\n{tf.reduce_min(X, axis=1)}\\n')\n",
    "print(f'{ANSI.BOLD}[tfp.stats.percentile]{ANSI.END} Compute percentile of the Tensor\\n{tfp.stats.percentile(X, 50, axis=1)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.pow, tf.log]{ANSI.END} Compute power or log of Tensor\\n{tf.pow(X, 2)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.squeeze]{ANSI.END} Remove dimensions with 1 length\\n{tf.squeeze(w)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.expand_dims]{ANSI.END} Expand dimension \\n{tf.expand_dims(tf.squeeze(w), axis=1)}\\n')\n",
    "print(f'{ANSI.BOLD}[tf.where]{ANSI.END} Replace values if condition is (not) met\\n{tf.where(X > 0, tf.zeros_like(X), X)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives, Jacobian Matrix and Gradient\n",
    "Suppose $\\mathbf{\\hat{y}}$ is an $m$ length vector, and $\\mathbf{\\hat{y}}$ is a function of another vector $\\mathbf{w}$ with length $n$ (i.e. $\\mathbf{\\hat{y}} = \\psi(\\mathbf{w})$, where $\\psi: \\mathbb{R}^n \\to \\mathbb{R}^m$). The __Jacobian matrix__ (matrix with first-order partial derivatives) of $\\mathbf{\\hat{y}}$ with respect to $\\mathbf{w}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}}=[\\hat{y}_{i}]=\\begin{bmatrix}\n",
    "    \\hat{y}_{1} \\\\\n",
    "    \\hat{y}_{2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    \\hat{y}_{m} \\\\\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{w}=[w_{j}]=\\begin{bmatrix}\n",
    "    w_{1} \\\\\n",
    "    w_{2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    w_{n} \\\\\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{\\frac{\\partial \\hat{y}}{\\partial w}}=\\begin{bmatrix}\n",
    "    \\frac{\\partial\\hat{y}_1}{\\partial w_1} & \\frac{\\partial\\hat{y}_1}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_1}{\\partial w_n} \\\\\n",
    "    \\frac{\\partial\\hat{y}_2}{\\partial w_1} & \\frac{\\partial\\hat{y}_2}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_2}{\\partial w_n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "    \\frac{\\partial\\hat{y}_m}{\\partial w_1} & \\frac{\\partial\\hat{y}_n}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_m}{\\partial w_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For instance, let $\\mathbf{\\hat{y}}=\\mathbf{Xw} + b$, where $\\mathbf{X}$ is a matrix independent from $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=[x_{i,j}]=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "    \\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{\\hat{y}}=\n",
    "\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    w_{1}   \\\\\n",
    "    w_{2}   \\\\\n",
    "    \\vdots  \\\\\n",
    "    w_{n}   \\\\\n",
    "\\end{bmatrix} + b=\n",
    "\\begin{bmatrix}\n",
    "    w_1x_{1,1} + w_2x_{1,2} + \\cdots + w_mx_{1,n} + b \\\\\n",
    "    w_1x_{2,1} + w_2x_{2,2} + \\cdots + w_mx_{2,n} + b \\\\\n",
    "    \\vdots \\\\\n",
    "    w_1x_{m,1} + w_2x_{m,2} + \\cdots + w_mx_{m,n} + b \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix of $\\mathbf{\\hat{y}}$ with respect to $\\mathbf{w}$ is:\n",
    "$$\n",
    "\\mathbf{\\frac{\\partial \\hat{y}}{\\partial w}}=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "    \\end{bmatrix} = \\mathbf{X}\n",
    "$$\n",
    "\n",
    "Suppose $f(\\mathbf{\\hat{y}})=\\mathbf{\\hat{y}}^T\\mathbf{A}\\mathbf{\\hat{y}}$, where $\\mathbf{A} \\in \\mathbb{R}^{m\\times m}$ is matrix independent from $\\mathbf{\\hat{y}}$, then $f(\\mathbf{\\hat{y}})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{\\hat{y}})&=\\begin{bmatrix} \\hat{y}_1 & \\hat{y}_2 & \\cdots & \\hat{y}_m \\\\ \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    a_{1,1} & a_{1,2} & \\cdots & a_{1,m}\\\\\n",
    "    a_{2,1} & a_{2,2} & \\cdots & a_{2,m}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    a_{m,1} & a_{m,2} & \\cdots & a_{m,m}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\begin{bmatrix} \\hat{y}_1 \\\\  \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_m \\\\ \\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix} \\sum\\limits_{i=1}^{m}a_{i,1}\\hat{y}_1 & \\sum\\limits_{i=1}^{m}a_{i,2}\\hat{y}_2 & \\cdots & \\sum\\limits_{i=1}^{m}a_{i,m}\\hat{y}_m \\\\ \\end{bmatrix}\\begin{bmatrix} \\hat{y}_1 \\\\  \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_m \\\\ \\end{bmatrix} \\\\\n",
    "&=\\sum\\limits_{j=1}^{m}\\hat{y}_j\\sum\\limits_{i=1}^{m}a_{i,j}\\hat{y}_j \\\\\n",
    "&=\\sum\\limits_{j=1}^{m}\\hat{y}_j^2\\sum\\limits_{i=1}^{m}a_{i,j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "then the __gradient__ (the partial derivatives of a sclar with respect to a vector) for this function $f$ with respect to $\\hat{y}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\hat{y}}f(\\hat{y})&=\\begin{bmatrix} \\frac{\\partial f}{\\partial\\hat{y}_1} & \\frac{\\partial f}{\\partial\\hat{y}_2} & \\cdots & \\frac{\\partial f}{\\partial\\hat{y}_m} \\end{bmatrix}^T \\\\\n",
    "&=\\begin{bmatrix} 2\\hat{y}_1\\sum\\limits_{i=1}^{m}a_{i,1} & 2\\hat{y}_2\\sum\\limits_{i=1}^{m}a_{i,2} & \\cdots & 2\\hat{y}_m\\sum\\limits_{i=1}^{m}a_{i,m} \\end{bmatrix}^T\\\\\n",
    "&=[2\\hat{y}^T\\mathbf{A}]^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Suppose we have another function that map an vector to a scalar ($L: \\mathbb{R}^m \\to \\mathbb{R}$), e.g. $L(\\mathbf{\\hat{y}})=(\\mathbf{y} - \\mathbf{\\hat{y}})^T(\\mathbf{y} - \\mathbf{\\hat{y}})$, where $\\mathbf{\\hat{y}}$ is a $m$ length vector independent from $\\mathbf{\\hat{y}}$ ($L$ is actually the square deviation between $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$)\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=[y_{i}]=\\begin{bmatrix}\n",
    "    y_{1} \\\\\n",
    "    y_{2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    y_{m} \\\\\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{\\hat{y}}=[\\hat{y}_{i}]=\\begin{bmatrix}\n",
    "    \\hat{y}_{1} \\\\\n",
    "    \\hat{y}_{2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    \\hat{y}_{m} \\\\\n",
    "\\end{bmatrix}=\\mathbf{Xw}+b, \\;\\;\\;\n",
    "L(\\mathbf{\\hat{y}})=\\sum\\limits_{i=1}^{m}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "We can compute the gradient of this function with respect to $\\mathbf{\\hat{y}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\hat{y}}L(\\hat{y})&=\\begin{bmatrix} \\frac{\\partial L}{\\partial\\hat{y_1}} & \\frac{\\partial L}{\\partial\\hat{y_2}} & \\cdots & \\frac{\\partial L}{\\partial\\hat{y_m}} \\end{bmatrix}^T \\\\\n",
    "&=\\begin{bmatrix} \n",
    "    \\frac{\\partial L}{\\partial(y_1 - \\hat{y}_1)}\\frac{\\partial(y_1 - \\hat{y}_1)}{\\partial\\hat{y}_1} & \n",
    "    \\frac{\\partial L}{\\partial(y_2 - \\hat{y}_2)}\\frac{\\partial(y_2 - \\hat{y}_2)}{\\partial\\hat{y}_2} & \n",
    "    \\cdots & \n",
    "    \\frac{\\partial L}{\\partial(y_m - \\hat{y}_m)}\\frac{\\partial(y_m - \\hat{y}_m)}{\\partial\\hat{y}_m} \\end{bmatrix}^T \\\\\n",
    "&=\\begin{bmatrix} \n",
    "    2(\\hat{y}_1 - y_1) & \n",
    "    2(\\hat{y}_2 - y_2) & \n",
    "    \\cdots & \n",
    "    2(\\hat{y}_m - y_m) \\end{bmatrix}^T\\\\\n",
    "&=-2(\\mathbf{y} - \\hat{y})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Moreover, we can compute the gradient of $L$ with respect to $\\mathbf{w}$ using multivariate chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{w}}L(\\hat{y})&=\\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} & \\frac{\\partial L}{\\partial w_2} & \\cdots & \\frac{\\partial L}{\\partial w_n} \\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_1} & \\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_2} & \\cdots &\\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_n} \n",
    "\\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix}\n",
    "    \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y_i}}\\frac{\\partial\\hat{y_i}}{\\partial w_1} & \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y_i}}\\frac{\\partial\\hat{y_i}}{\\partial w_2} & \\cdots & \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y_i}}\\frac{\\partial\\hat{y_i}}{\\partial w_n} \n",
    "\\end{bmatrix}^T\\\\\n",
    "&=[-2(\\mathbf{y} - \\hat{y})^T\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w}]^T\\\\\n",
    "&=[-2(\\mathbf{y} - \\hat{y})^T\\mathbf{X}]^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Are auto-computed gradient and self-derived gradient the same?: True\n"
    }
   ],
   "source": [
    "def allclose(x, y, rtol=1e-5, atol=1e-8):\n",
    "    return tf.reduce_all(tf.abs(x - y) <= tf.abs(y) * rtol + atol).numpy()\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "y = tf.Variable(tf.random.uniform((4, 1), minval=-5, maxval=5))\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    y_hat = tf.matmul(X, w) + b\n",
    "    loss = tf.reduce_sum(tf.pow(y - y_hat, 2))\n",
    "tf_autograd = g.gradient(loss, w)\n",
    "derived_grad = tf.transpose(-2 * tf.matmul(tf.transpose(y - y_hat), X))\n",
    "\n",
    "print(f'Are auto-computed gradient and self-derived gradient the same?: {allclose(derived_grad, tf_autograd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595991319202"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}